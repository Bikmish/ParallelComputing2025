Краткое руководство по решателю с использованием OpenMP, MPI и CUDA

АВТОР: Биктимиров Михаил Геннадьевич, группа 627
КУРС: "Суперкомпьютерное моделирование и технологии", МГУ ВМК, 2025

=== Структура проекта ===

Проект содержит 4 папки с реализациями для разных технологий параллельных вычислений:
- task1_OMP - многопоточная реализация, используя OpenMP
- task2_MPI - распределенная реализация с передачей сообщений с использованием MPI
- task3_MPI_OMP - гибридная реализация MPI+OpenMP
- task4_MPI_CUDA - реализация, используя гибридную версию CUDA+MPI (вычисления перенесены на ГПУ)


Для каждого проекта написан Makefile, в котором прописаны команды компиляции и запусков индивидуально для каждого задания.
Запуски проводились во всевозможных конфигурациях, статистика которых в отчёте к заданию отражена на 
соответствующих таблицах и графиках (таблицы 1-15 и графики 1-12 отчёта).
(ниже для каждой версии программы перечислены всевозможные конфигурации запусков)

=== Компиляция и запуск ===

---OpenMP---
	+ компиляция
		- локально: clang++ -O3 -std=c++11 -fopenmp solver.cpp main.cpp -o solver
			- или через Makefile: make build (компиляция локально с clang++)
		- на Polus: g++ -O3 -std=c++11 -fopenmp solver.cpp main.cpp -o solver
		- или через Makefile: make build_polus (компиляция на Polus с g++)
		
	+ тестирование и сбор статистики для отчёта:
		make test_n256_l1_thAll (N=256, L=1, потоки: 1,2,4,8,16,32)
		make test_n256_lPi_thAll (N=256, L=Pi, потоки: 1,2,4,8,16,32)
		make test_full_config (N:128,256,512; L:1,Pi; потоки:1,2,4,8,16,32)
		make test_n256_l1_thAll_polus (N=256, L=1, потоки:1,2,4,8,16,32)
		make test_full_config_polus (N:128,256,512; L:1,Pi; потоки:1,2,4,8,16,32)
		
Для полноты отчёта и корректности выводов для данной версии и всех последующих были
проведены всевозможные запуски с разными конфигурациями параметров и разной привязкой с последующем усреднением статистики

---MPI---
	+ компиляция:
		- mpixlC -O3 -std=c++11 -o solverMPI_only main.cpp
		- через Makefile: make build или просто make
	+ тестирование и сбор статистики для отчёта:
		make run_all (N:128,256,512; L:1,Pi; процессы:1,2,4,8,16,32)
		make clean (очистка результатов)
		make help (справка по командам)
		
---MPI+OpenMP---
	+ компиляция:
		- mpixlC -O3 -qsmp=omp -std=c++11 -o solverMPI main.cpp
		- или через Makefile: make build или просто make
	+ тестирование и сбор статистики для отчёта:
		make run_all (N:128,256,512; L:1,Pi; MPI:4,8 проц; OMP:1,2,4,8 потоков)
		make run_all_1 (N:128,256,512; L:1,Pi; MPI:1,4,8,16,32 проц; OMP:1 поток)
		make run_all_max (N:128,256,512; L:1,Pi; MPI:8 проц; OMP:20 потоков)
		make clean (очистка результатов)
		make help (справка по командам)

---CUDA+MPI---
	+ компиляция:
		- nvcc -O3 -arch=sm_35 -ccbin mpic++ -o solverCUDA main.cu -x cu -Xcompiler -Wall -std=c++11
		- или через Makefile: make build
	+ тестирование и сбор статистики для отчёта:
		make run_test (N=256, L=Pi, MPI:8 проц, 1 GPU shared) - тестовый запуск
		make run_all_single_gpu (N:128,256,512; L=Pi; MPI:1,2,4,8,16 проц; 1 GPU shared) - shared-режим
		make run_all_single_gpu_ex (N:128,256,512; L=Pi; MPI:1,2,4,8,16 проц; 1 GPU exclusive) - эксклюзивный режим
		make run_all_double_gpu (N:128,256,512; L=Pi; MPI:1,2,4,8,16 проц; 2 GPU shared) - shared-режим
		make run_all_double_gpu_ex (N:128,256,512; L=Pi; MPI:1,2,4,8,16 проц; 2 GPU exclusive) - эксклюзивный режим
		make clean (очистка)

Для CUDA-реализации главным образом сравнивались конфигурации "1 ГПУ + 1 процесс" и "2 ГПУ + 2 процесс", остальные
конфигурации запускались чисто из научного интереса
		
Подробный анализ статистики, выводы, таблицы, графики и подробности реализаций каждой из версий программ описаны в 
прикреплённом к архиву отчёту. Также для сдачи CUDA+MPI части задания был создан github-репозиторий, 
включающий исходный код всех версий описанных в отчёте программ:
https://github.com/Bikmish/ParallelComputing2025